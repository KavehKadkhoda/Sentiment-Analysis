{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7_NaiveBayes_for_sentiment_analysis_on_tweets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMQTGDI3XvA7zpP0H9r1/Ny",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KavehKadkhoda/Sentiment-Analysis/blob/main/7_NaiveBayes_for_sentiment_analysis_on_tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M_a620WFXlIV"
      },
      "outputs": [],
      "source": [
        "# Train a naive bayes model on a sentiment analysis task\n",
        "# Test using your model\n",
        "# Compute ratios of positive words to negative words\n",
        "# Do some error analysis\n",
        "# Predict on your own tweet\n",
        "\n",
        "# More info:\n",
        "# https://docs.google.com/document/d/1s7mi6aRC2anRmvK5AbZQiQSUwDMRVVHdM0MLkPcFXrM/edit?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdb\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from os import getcwd\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTWwY7E6X9Lb",
        "outputId": "a7b8f2af-56eb-43e7-bf7b-8027f5dab088"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "from matplotlib.patches import Ellipse\n",
        "import matplotlib.transforms as transforms\n",
        "\n",
        "import numpy as np # Library for linear algebra and math utils\n",
        "\n",
        "def process_tweet(tweet):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "\n",
        "    '''\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    #tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "            word not in string.punctuation):  # remove punctuation\n",
        "            # tweets_clean.append(word)\n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean"
      ],
      "metadata": {
        "id": "EP6dnT-MYb8Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lookup(freqs, word, label):\n",
        "    '''\n",
        "    Input:\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        word: the word to look up\n",
        "        label: the label corresponding to the word\n",
        "    Output:\n",
        "        n: the number of times the word with its corresponding label appears.\n",
        "    '''\n",
        "    n = 0  # freqs.get((word, label), 0)\n",
        "\n",
        "    pair = (word, label)\n",
        "    if (pair in freqs):\n",
        "        n = freqs[pair]\n",
        "\n",
        "    return n"
      ],
      "metadata": {
        "id": "WqZtmE_hYfdn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the sets of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "# split the data into two pieces, one for training and one for testing (validation set)\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg\n",
        "\n",
        "# avoid assumptions about the length of all_positive_tweets\n",
        "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
        "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
      ],
      "metadata": {
        "id": "mpiRDnLVYwJt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 1: Process the Data\n",
        "# For any machine learning project, once you've gathered the data, the first step is to process it to make useful inputs to your model.\n",
        "\n",
        "# Remove noise: You will first want to remove noise from your data -- that is, remove words that don't tell you much about the content. \n",
        "# These include all common words like 'I, you, are, is, etc...' that would not give us enough information on the sentiment.\n",
        "# We'll also remove stock market tickers, retweet symbols, hyperlinks, \n",
        "# and hashtags because they can not tell you a lot of information on the sentiment.\n",
        "# You also want to remove all the punctuation from a tweet. \n",
        "# The reason for doing this is because we want to treat words with or without the punctuation as the same word, \n",
        "# instead of treating \"happy\", \"happy?\", \"happy!\", \"happy,\" and \"happy.\" as different words.\n",
        "# Finally you want to use stemming to only keep track of one variation of each word. In other words, \n",
        "# we'll treat \"motivation\", \"motivated\", and \"motivate\" similarly by grouping them within the same stem of \"motiv-\".\n",
        "# We have given you the function process_tweet that does this for you."
      ],
      "metadata": {
        "id": "zD20WSQXY8SH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
        "\n",
        "# print cleaned tweet\n",
        "print(process_tweet(custom_tweet))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yS1jf8qZQ8x",
        "outputId": "bb5126d3-9164-40f7-ff63-cf9b7e4fcda2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'great', 'day', ':)', 'good', 'morn']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 1.1 Implementing your helper functions\n",
        "# To help you train your naive bayes model, you will need to compute a dictionary \n",
        "# where the keys are a tuple (word, label) and the values are the corresponding frequency. \n",
        "# Note that the labels we'll use here are 1 for positive and 0 for negative.\n",
        "\n",
        "# You will also implement a lookup helper function that takes in the freqs dictionary, \n",
        "# a word, and a label (1 or 0) and returns the number of times that word and label tuple appears in the collection of tweets.\n",
        "\n",
        "# For example: given a list of tweets [\"i am rather excited\", \"you are rather happy\"] and the label 1, the function will return a dictionary that contains the following key-value pairs:\n",
        "\n",
        "# { (\"rather\", 1): 2, (\"happi\", 1) : 1, (\"excit\", 1) : 1 }\n",
        "\n",
        "# Notice how for each word in the given string, the same label 1 is assigned to each word.\n",
        "# Notice how the words \"i\" and \"am\" are not saved, since it was removed by process_tweet because it is a stopword.\n",
        "# Notice how the word \"rather\" appears twice in the list of tweets, and so its count value is 2.\n",
        "\n",
        "# Create a function count_tweets that takes a list of tweets as input, cleans all of them, and returns a dictionary.\n",
        "\n",
        "# The key in the dictionary is a tuple containing the stemmed word and its class label, e.g. (\"happi\",1).\n",
        "# The value the number of times this word appears in the given collection of tweets (an integer)."
      ],
      "metadata": {
        "id": "lJ89DUNKZr--"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_tweets(result, tweets, ys):\n",
        "    '''\n",
        "    Input:\n",
        "        result: a dictionary that will be used to map each pair to its frequency\n",
        "        tweets: a list of tweets\n",
        "        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n",
        "    Output:\n",
        "        result: a dictionary mapping each pair to its frequency\n",
        "    '''\n",
        "    for y, tweet in zip(ys, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            # define the key, which is the word and label tuple\n",
        "            pair = (word, y)\n",
        "            \n",
        "            # if the key exists in the dictionary, increment the count\n",
        "            if pair in result:\n",
        "                result[pair] += 1\n",
        "\n",
        "            # else, if the key is new, add it to the dictionary and set the count to 1\n",
        "            else:\n",
        "                result[pair] = 1\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "rzmgWzI6aeyl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing your function\n",
        "\n",
        "result = {}\n",
        "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
        "ys = [1, 0, 0, 0, 0]\n",
        "count_tweets(result, tweets, ys)\n",
        "\n",
        "# Expected Output: {('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_K2sYApbLeV",
        "outputId": "6229d171-0542-4874-abc5-b2a8f48a204e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('happi', 1): 1, ('sad', 0): 1, ('tire', 0): 2, ('trick', 0): 1}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 2: Train your model using Naive Bayes Naive bayes is an algorithm that could be used for sentiment analysis. \n",
        "# It takes a short time to train and also has a short prediction time.\n",
        "\n",
        "# Build the freqs dictionary for later uses\n",
        "freqs = count_tweets({}, train_x, train_y)"
      ],
      "metadata": {
        "id": "LykqA7lLbaB5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_naive_bayes(freqs, train_x, train_y):\n",
        "    '''\n",
        "    Input:\n",
        "        freqs: dictionary from (word, label) to how often the word appears\n",
        "        train_x: a list of tweets\n",
        "        train_y: a list of labels correponding to the tweets (0,1)\n",
        "    Output:\n",
        "        logprior: the log prior. (equation 3 above)\n",
        "        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
        "    '''\n",
        "    loglikelihood = {}\n",
        "    logprior = 0\n",
        "\n",
        "\n",
        "    # calculate V, the number of unique words in the vocabulary\n",
        "    vocab = set([pair[0] for pair in freqs.keys()])\n",
        "    V = len(vocab)    \n",
        "\n",
        "    # calculate N_pos, N_neg, V_pos, V_neg\n",
        "    N_pos = N_neg = 0\n",
        "    for pair in freqs.keys():\n",
        "        # if the label is positive (greater than zero)\n",
        "        if pair[1] > 0:\n",
        "\n",
        "            # Increment the number of positive words by the count for this (word, label) pair\n",
        "            N_pos += freqs[pair]\n",
        "\n",
        "        # else, the label is negative\n",
        "        else:\n",
        "\n",
        "            # increment the number of negative words by the count for this (word,label) pair\n",
        "            N_neg += freqs[pair]\n",
        "    \n",
        "    # Calculate D, the number of documents\n",
        "    D = len(train_y)\n",
        "\n",
        "    # Calculate D_pos, the number of positive documents\n",
        "    D_pos = len(list(filter(lambda x:x>0, train_y)))\n",
        "\n",
        "    # Calculate D_neg, the number of negative documents\n",
        "    D_neg = D - D_pos\n",
        "\n",
        "    # Calculate logprior\n",
        "    logprior = np.log(D_pos) - np.log(D_neg)\n",
        "    \n",
        "    # For each word in the vocabulary...\n",
        "    for word in vocab:\n",
        "        # get the positive and negative frequency of the word\n",
        "        freq_pos = lookup(freqs, word, 1)\n",
        "        freq_neg = lookup(freqs, word, 0)\n",
        "\n",
        "        # calculate the probability that each word is positive, and negative\n",
        "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
        "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
        "\n",
        "        # calculate the log likelihood of the word\n",
        "        loglikelihood[word] = np.log(p_w_pos / p_w_neg)\n",
        "\n",
        "\n",
        "    return logprior, loglikelihood"
      ],
      "metadata": {
        "id": "oHh2H14Bejeu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
        "print(logprior)\n",
        "print(len(loglikelihood))\n",
        "\n",
        "# Expected Output:\n",
        "# 0.0\n",
        "# 9165"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkFAQl2iepAk",
        "outputId": "329c9492-ffc7-4ddb-f6e7-685f3fb39ab8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "9165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 3: Test your naive bayes \n",
        "# Now that we have the logprior and loglikelihood, we can test the naive bayes function by making predicting on some tweets!\n",
        "\n",
        "# Note\n",
        "# Note we calculate the prior from the training data, and that the training data is evenly split between positive and negative labels\n",
        "#  (4000 positive and 4000 negative tweets). This means that the ratio of positive to negative 1, and the logprior is 0.\n",
        "\n",
        "# The value of 0.0 means that when we add the logprior to the log likelihood, \n",
        "# we're just adding zero to the log likelihood. However, please remember to include the logprior, \n",
        "# because whenever the data is not perfectly balanced, the logprior will be a non-zero value."
      ],
      "metadata": {
        "id": "uCji_CGK65ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# naive_bayes_predict\n",
        "\n",
        "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string\n",
        "        logprior: a number\n",
        "        loglikelihood: a dictionary of words mapping to numbers\n",
        "    Output:\n",
        "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
        "\n",
        "    '''\n",
        "    # process the tweet to get a list of words\n",
        "    word_l = process_tweet(tweet)\n",
        "\n",
        "    # initialize probability to zero\n",
        "    p = 0\n",
        "\n",
        "    # add the logprior\n",
        "    p += logprior\n",
        "\n",
        "    for word in word_l:\n",
        "\n",
        "        # check if the word exists in the loglikelihood dictionary\n",
        "        if word in loglikelihood:\n",
        "            # add the log likelihood of that word to the probability\n",
        "            p += loglikelihood[word]\n",
        "\n",
        "\n",
        "    return p"
      ],
      "metadata": {
        "id": "FZKGFjLv7SLK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment with your own tweet.\n",
        "my_tweet = 'She smiled.'\n",
        "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
        "print('The expected output is', p)\n",
        "\n",
        "# Expected Output:\n",
        "# The expected output is around 1.55\n",
        "# The sentiment is positive."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh2WJuhf7-6_",
        "outputId": "e15f7ef1-c339-4f58-a34b-fba3a7f0ebd7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The expected output is 1.5577981920239676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement test_naive_bayes\n",
        "\n",
        "# Instructions:\n",
        "# Implement test_naive_bayes to check the accuracy of your predictions.\n",
        "# The function takes in your test_x, test_y, log_prior, and loglikelihood\n",
        "# It returns the accuracy of your model.\n",
        "# First, use naive_bayes_predict function to make predictions for each tweet in text_x."
      ],
      "metadata": {
        "id": "FLjsd7ly8IGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_naive_bayes\n",
        "\n",
        "def test_naive_bayes(test_x, test_y, logprior, loglikelihood, naive_bayes_predict=naive_bayes_predict):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        test_x: A list of tweets\n",
        "        test_y: the corresponding labels for the list of tweets\n",
        "        logprior: the logprior\n",
        "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
        "    Output:\n",
        "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
        "    \"\"\"\n",
        "    accuracy = 0  # return this properly\n",
        "\n",
        "    y_hats = []\n",
        "    for tweet in test_x:\n",
        "        # if the prediction is > 0\n",
        "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
        "            # the predicted class is 1\n",
        "            y_hat_i = 1\n",
        "        else:\n",
        "            # otherwise the predicted class is 0\n",
        "            y_hat_i = 0\n",
        "\n",
        "        # append the predicted class to the list y_hats\n",
        "        y_hats.append(y_hat_i)\n",
        "\n",
        "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
        "    error = np.mean(np.absolute(y_hats - test_y))\n",
        "\n",
        "    # Accuracy is 1 minus the error\n",
        "    accuracy = 1 - error\n",
        "                    \n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "ofswMWp48T3S"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Naive Bayes accuracy = %0.4f\" %\n",
        "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))\n",
        "\n",
        "# Expected Accuracy:\n",
        "\n",
        "# Naive Bayes accuracy = 0.9955"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSoDGQ4P9Y84",
        "outputId": "0c551289-9b44-458a-946a-0cf65f82eabb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes accuracy = 0.9955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to test your function\n",
        "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:    \n",
        "    p = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
        "    print(f'{tweet} -> {p:.2f}')\n",
        "\n",
        "# Expected Output:\n",
        "# I am happy -> 2.14\n",
        "# I am bad -> -1.31\n",
        "# this movie should have been great. -> 2.12\n",
        "# great -> 2.13\n",
        "# great great -> 4.26\n",
        "# great great great -> 6.39\n",
        "# great great great great -> 8.52"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2MQoBu39hHe",
        "outputId": "eb7278c3-b2b1-4230-e1bc-599c5e2a5791"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am happy -> 2.14\n",
            "I am bad -> -1.31\n",
            "this movie should have been great. -> 2.12\n",
            "great -> 2.13\n",
            "great great -> 4.26\n",
            "great great great -> 6.39\n",
            "great great great great -> 8.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to check the sentiment of your own tweet below\n",
        "my_tweet = 'Stupidity is knowing the truth, seeing the truth but still believing the lies. And that is more infectious than any other disease.'\n",
        "naive_bayes_predict(my_tweet, logprior, loglikelihood)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkmslFpt9wbL",
        "outputId": "778d56fd-a8d4-4892-9947-c641e23a6464"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-3.2399026615032165"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 4: Filter words by Ratio of positive to negative counts¶\n",
        "# Some words have more positive counts than others, and can be considered \"more positive\". \n",
        "# Likewise, some words can be considered more negative than others.\n",
        "# One way for us to define the level of positiveness or negativeness, \n",
        "# without calculating the log likelihood, is to compare the positive to negative frequency of the word.\n",
        "# Note that we can also use the log likelihood calculations to compare relative positivity or negativity of words.\n",
        "# We can calculate the ratio of positive to negative frequencies of a word.\n",
        "# Once we're able to calculate these ratios, we can also filter a subset of words that have a minimum ratio of positivity / negativity or higher.\n",
        "# Similarly, we can also filter a subset of words \n",
        "# that have a maximum ratio of positivity / negativity or lower (words that are at least as negative, or even more negative than a given threshold)."
      ],
      "metadata": {
        "id": "s65s1HhD-Oim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement get_ratio()\n",
        "# Given the freqs dictionary of words and a particular word, use lookup(freqs,word,1) to get the positive count of the word.\n",
        "# Similarly, use the lookup() function to get the negative count of that word.\n",
        "# Calculate the ratio of positive divided by negative counts\n",
        "\n",
        "# get_ratio\n",
        "\n",
        "def get_ratio(freqs, word):\n",
        "    '''\n",
        "    Input:\n",
        "        freqs: dictionary containing the words\n",
        "\n",
        "    Output: a dictionary with keys 'positive', 'negative', and 'ratio'.\n",
        "        Example: {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
        "    '''\n",
        "    pos_neg_ratio = {'positive': 0, 'negative': 0, 'ratio': 0.0}\n",
        "    ### START CODE HERE ###\n",
        "    # use lookup() to find positive counts for the word (denoted by the integer 1)\n",
        "    pos_neg_ratio['positive'] = lookup(freqs, word, 1)\n",
        "    \n",
        "    # use lookup() to find negative counts for the word (denoted by integer 0)\n",
        "    pos_neg_ratio['negative'] = lookup(freqs, word, 0)\n",
        "    \n",
        "    # calculate the ratio of positive to negative counts for the word\n",
        "    pos_neg_ratio['ratio'] = (\n",
        "        pos_neg_ratio['positive'] + 1) / (pos_neg_ratio['negative'] + 1)\n",
        "    ### END CODE HERE ###\n",
        "    return pos_neg_ratio"
      ],
      "metadata": {
        "id": "cimxTAGt-rGU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement get_words_by_threshold(freqs,label,threshold)\n",
        "# If we set the label to 1, then we'll look for all words whose threshold of positive/negative is at least as high as that threshold, or higher.\n",
        "# If we set the label to 0, then we'll look for all words whose threshold of positive/negative is at most as low as the given threshold, or lower.\n",
        "# Use the get_ratio function to get a dictionary containing the positive count, negative count, and the ratio of positive to negative counts.\n",
        "# Append the get_ratio dictionary inside another dictinoary, \n",
        "# where the key is the word, and the value is the dictionary pos_neg_ratio that is returned by the get_ratio function. \n",
        "# An example key-value pair would have this structure:\n",
        "# {'happi':{'positive': 10, 'negative': 20, 'ratio': 0.524}}"
      ],
      "metadata": {
        "id": "GpjRATYl_ITe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get_words_by_threshold\n",
        "\n",
        "def get_words_by_threshold(freqs, label, threshold, get_ratio=get_ratio):\n",
        "    '''\n",
        "    Input:\n",
        "        freqs: dictionary of words\n",
        "        label: 1 for positive, 0 for negative\n",
        "        threshold: ratio that will be used as the cutoff for including a word in the returned dictionary\n",
        "    Output:\n",
        "        word_list: dictionary containing the word and information on its positive count, negative count, and ratio of positive to negative counts.\n",
        "        example of a key value pair:\n",
        "        {'happi':\n",
        "            {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
        "        }\n",
        "    '''\n",
        "    word_list = {}\n",
        "    # CODE REVIEW COMMENT: This has been changed!! word_list was described as a dictionary, but defined (and operated on) as a list\n",
        "\n",
        "    for key in freqs.keys():\n",
        "        word, _ = key\n",
        "\n",
        "        # get the positive/negative ratio for a word\n",
        "        pos_neg_ratio = get_ratio(freqs, word)\n",
        "\n",
        "        # if the label is 1 and the ratio is greater than or equal to the threshold...\n",
        "        if label == 1 and pos_neg_ratio['ratio'] >= threshold:\n",
        "        \n",
        "            # Add the pos_neg_ratio to the dictionary\n",
        "            word_list[word] = pos_neg_ratio\n",
        "\n",
        "        # If the label is 0 and the pos_neg_ratio is less than or equal to the threshold...\n",
        "        elif label == 0 and pos_neg_ratio['ratio'] <= threshold:\n",
        "        \n",
        "            # Add the pos_neg_ratio to the dictionary\n",
        "            word_list[word] = pos_neg_ratio\n",
        "\n",
        "        # otherwise, do not include this word in the list (do nothing)\n",
        "\n",
        "    return word_list\n"
      ],
      "metadata": {
        "id": "TomsGexR_Zic"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 5: Error Analysis\n",
        "# In this part you will see some tweets that your model missclassified. \n",
        "# Why do you think the missclassifications happened? Were there any assumptions made by your naive bayes model?"
      ],
      "metadata": {
        "id": "__2NrrJyAN-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some error analysis done for you\n",
        "print('Truth Predicted Tweet')\n",
        "for x, y in zip(test_x, test_y):\n",
        "    y_hat = naive_bayes_predict(x, logprior, loglikelihood)\n",
        "    if y != (np.sign(y_hat) > 0):\n",
        "        print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n",
        "            process_tweet(x)).encode('ascii', 'ignore')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SdisDdiAVxW",
        "outputId": "817ff167-a112-41b3-9c6b-606e09e18f8b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Truth Predicted Tweet\n",
            "1\t0.00\tb'truli later move know queen bee upward bound movingonup'\n",
            "1\t0.00\tb'new report talk burn calori cold work harder warm feel better weather :p'\n",
            "1\t0.00\tb'harri niall 94 harri born ik stupid wanna chang :D'\n",
            "1\t0.00\tb'park get sunlight'\n",
            "1\t0.00\tb'uff itna miss karhi thi ap :p'\n",
            "0\t1.00\tb'hello info possibl interest jonatha close join beti :( great'\n",
            "0\t1.00\tb'u prob fun david'\n",
            "0\t1.00\tb'pat jay'\n",
            "0\t1.00\tb'sr financi analyst expedia inc bellevu wa financ expediajob job job hire'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 6: Predict with your own tweet\n",
        "# In this part you can predict the sentiment of your own tweet."
      ],
      "metadata": {
        "id": "5x2WUOZdAdzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with your own tweet - feel free to modify `my_tweet`\n",
        "my_tweet = 'If your theory doesnt agree with experiment, its wrong. It doesnt matter how beautiful it is. Brain'\n",
        "\n",
        "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
        "print(p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGPeKxEPAkHu",
        "outputId": "1f9792ec-e7ab-46cb-a5e5-9aa607c81134"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.048583353113135\n"
          ]
        }
      ]
    }
  ]
}
