{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8_Find_tweets_that_are_similar_to_a_given_tweet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMIb5qwtoaOvG0U2iomuett",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KavehKadkhoda/Sentiment-Analysis/blob/main/8_Find_tweets_that_are_similar_to_a_given_tweet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y8omen2RHXg_"
      },
      "outputs": [],
      "source": [
        "# LSH and document search\n",
        "# We will implement a more efficient version of k-nearest neighbors using locality sensitive hashing. \n",
        "# I will then apply this to document search.\n",
        "\n",
        "# Process the tweets and represent each tweet as a vector (represent a document with a vector embedding).\n",
        "# Use locality sensitive hashing and k nearest neighbors to find tweets that are similar to a given tweet."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  positive_tweets.json\n",
        "#  negative_tweets.json\n",
        "\n",
        "# These were collected in July 2015 by searching against the following strings:\n",
        "\n",
        "# positive\n",
        "# -------\n",
        "#     ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
        "#     ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
        "#     '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
        "#     'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
        "#     '<3'\n",
        "#     }\n",
        "\n",
        "# negative\n",
        "# ------\n",
        "#     ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
        "#     ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
        "#     ':c', ':{', '>:\\\\', ';('\n",
        "   \n"
      ],
      "metadata": {
        "id": "5GDKD1IlIk9J"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The sample dataset from NLTK is separated into positive and negative tweets. \n",
        "#It contains 5000 positive tweets and 5000 negative tweets exactly.\n",
        "#It is just because balanced datasets simplify the design of most computational methods that are required for sentiment analysis. \n",
        "#You can download the dataset in your workspace (or in your local computer) by doing:\n",
        "\n",
        "import nltk # Python library for NLP\n",
        "\n",
        "# downloads sample twitter dataset.\n",
        "nltk.download('twitter_samples')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LUX7YVPakRw",
        "outputId": "36cc6c70-3ae9-4ff2-91ca-762612f20ff8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
        "\n",
        "\n",
        "# get the positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "all_tweets = all_positive_tweets + all_negative_tweets"
      ],
      "metadata": {
        "id": "A-7HkP1iZPEq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Getting the document embeddings\n",
        "# Bag-of-words (BOW) document models\n",
        "# Text documents are sequences of words.\n",
        "\n",
        "# The ordering of words makes a difference. For example, sentences \"Apple pie is better than pepperoni pizza.\"\n",
        "# and \"Pepperoni pizza is better than apple pie\" have opposite meanings due to the word ordering.\n",
        "# However, for some applications, ignoring the order of words can allow us to train an efficient and still effective model.\n",
        "# This approach is called Bag-of-words document model.\n",
        "\n",
        "# Document embeddings\n",
        "# Document embedding is created by summing up the embeddings of all words in the document.\n",
        "# If we don't know the embedding of some word, we can ignore that word.\n",
        "\n",
        "\n",
        "# The function get_document_embedding() encodes entire document as a \"document\" embedding.\n",
        "# It takes in a document (as a string) and a dictionary, en_embeddings\n",
        "# It processes the document, and looks up the corresponding embedding of each word.\n",
        "# It then sums them up and returns the sum of all word vectors of that processed tweet."
      ],
      "metadata": {
        "id": "ZAV_KRJV3aaQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_tweet(tweet):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "\n",
        "    '''\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "            word not in string.punctuation):  # remove punctuation\n",
        "            # tweets_clean.append(word)\n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean"
      ],
      "metadata": {
        "id": "wbf6Z2OM2ZWC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_document_embedding(tweet, en_embeddings, process_tweet=process_tweet):\n",
        "    '''\n",
        "    Input:\n",
        "        - tweet: a string\n",
        "        - en_embeddings: a dictionary of word embeddings\n",
        "    Output:\n",
        "        - doc_embedding: sum of all word embeddings in the tweet\n",
        "    '''\n",
        "    doc_embedding = np.zeros(300)\n",
        "\n",
        "    # process the document into a list of words (process the tweet)\n",
        "    processed_doc = process_tweet(tweet)\n",
        "    for word in processed_doc:\n",
        "        # add the word embedding to the running total for the document embedding\n",
        "        if word in en_embeddings.keys():\n",
        "            word_embedding = en_embeddings.get(word)\n",
        "        else:\n",
        "            word_embedding = 0\n",
        "            \n",
        "        doc_embedding = doc_embedding + word_embedding\n",
        "\n",
        "    return doc_embedding"
      ],
      "metadata": {
        "id": "C8OjlCpK4u08"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Colaboratory, or \"Colab\" for short, allows you to write and execute Python in your browser.\n",
        "#https://colab.research.google.com\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rwZ4aiDj0RH",
        "outputId": "f0c2a9c3-8d43-43ba-97f3-fe5b20c5e4b9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  The word embeddings data for English and French words\n",
        "\n",
        "# The data\n",
        "# The full dataset for English embeddings is about 3.64 gigabytes, \n",
        "# and the French embeddings are about 629 megabytes. \n",
        "# To prevent the workspace from crashing, we've extracted a subset of the embeddings for the words that you'll use in this assignment.\n",
        "\n",
        "\n",
        "#DATA: Add shortcut to your google drive\n",
        "\n",
        "# en_embeddings.p :\n",
        "# https://drive.google.com/file/d/1WBZeg8xNSHc27IIKQO5tJ1hNqO88TkAz/view?usp=sharing\n",
        "\n",
        "# fr_embeddings.p :\n",
        "# https://drive.google.com/file/d/1nzYKnN2bWCPeL3F_bJEX7KPHSNPqYMua/view?usp=sharing\n",
        "\n",
        "import pandas as pd\n",
        "en_embeddings_subset = pd.read_pickle(r'drive/MyDrive/NLP/en_embeddings.p')\n",
        "fr_embeddings_subset = pd.read_pickle(r'drive/MyDrive/NLP/fr_embeddings.p')\n",
        "\n",
        "# en_embeddings_subset = pickle.load(open(\"drive/MyDrive/NLP/en_embeddings.p\", \"rb\"))\n",
        "# fr_embeddings_subset = pickle.load(open(\"drive/MyDrive/NLP/fr_embeddings.p\", \"rb\"))"
      ],
      "metadata": {
        "id": "8wm5UOLZ_1sl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "import nltk  # Python library for NLP\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "# testing your function\n",
        "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
        "\n",
        "tweet_embedding = get_document_embedding(custom_tweet, en_embeddings_subset)\n",
        "tweet_embedding[-5:]\n",
        "\n",
        "# Expected output:\n",
        "# array([-0.00268555, -0.15378189, -0.55761719, -0.07216644, -0.32263184])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKy8s8MP9Dkc",
        "outputId": "74281007-14dd-4b61-b7db-083a3cbbc4a7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.00268555, -0.15378189, -0.55761719, -0.07216644, -0.32263184])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Store all document vectors into a dictionary\n",
        "# Now, let's store all the tweet embeddings into a dictionary. Implement get_document_vecs()"
      ],
      "metadata": {
        "id": "YbtP1lm-Vy8M"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_document_vecs(all_docs, en_embeddings, get_document_embedding=get_document_embedding):\n",
        "    '''\n",
        "    Input:\n",
        "        - all_docs: list of strings - all tweets in our dataset.\n",
        "        - en_embeddings: dictionary with words as the keys and their embeddings as the values.\n",
        "    Output:\n",
        "        - document_vec_matrix: matrix of tweet embeddings.\n",
        "        - ind2Doc_dict: dictionary with indices of tweets in vecs as keys and their embeddings as the values.\n",
        "    '''\n",
        "\n",
        "    # the dictionary's key is an index (integer) that identifies a specific tweet\n",
        "    # the value is the document embedding for that document\n",
        "    ind2Doc_dict = {}\n",
        "\n",
        "    # this is list that will store the document vectors\n",
        "    document_vec_l = []\n",
        "\n",
        "    for i, doc in enumerate(all_docs):\n",
        "\n",
        "\n",
        "        # get the document embedding of the tweet\n",
        "        doc_embedding = get_document_embedding(doc, en_embeddings)\n",
        "\n",
        "        # save the document embedding into the ind2Tweet dictionary at index i\n",
        "        ind2Doc_dict[i] = doc_embedding\n",
        "\n",
        "        # append the document embedding to the list of document vectors\n",
        "        document_vec_l.append(ind2Doc_dict[i])\n",
        "\n",
        "\n",
        "\n",
        "    # convert the list of document vectors into a 2D array (each row is a document vector)\n",
        "    document_vec_matrix = np.vstack(document_vec_l)\n",
        "\n",
        "    return document_vec_matrix, ind2Doc_dict"
      ],
      "metadata": {
        "id": "IkiLe4a9_Otb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_vecs, ind2Tweet = get_document_vecs(all_tweets, en_embeddings_subset)"
      ],
      "metadata": {
        "id": "gad1ACkEaAg_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"length of dictionary {len(ind2Tweet)}\")\n",
        "print(f\"shape of document_vecs {document_vecs.shape}\")\n",
        "\n",
        "# Expected Output\n",
        "# length of dictionary 10000\n",
        "# shape of document_vecs (10000, 300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0FjD53Qa9lc",
        "outputId": "2eb601a0-163d-41e6-ede2-02d5f68b1a94"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dictionary 10000\n",
            "shape of document_vecs (10000, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Looking up the tweets\n",
        "# Now you will input a tweet, and use cosine similarity to see which tweet in our corpus is similar to your tweet."
      ],
      "metadata": {
        "id": "vTZhPYFghVrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_tweet = 'All good work is done the way ants do things, little by little'\n",
        "process_tweet(my_tweet)\n",
        "tweet_embedding = get_document_embedding(my_tweet, en_embeddings_subset)"
      ],
      "metadata": {
        "id": "P_VvwHA9hipA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(A, B):\n",
        "    '''\n",
        "    Input:\n",
        "        A: a numpy array which corresponds to a word vector\n",
        "        B: A numpy array which corresponds to a word vector\n",
        "    Output:\n",
        "        cos: numerical number representing the cosine similarity between A and B.\n",
        "    '''\n",
        "    # you have to set this variable to the true label.\n",
        "    cos = -10\n",
        "    dot = np.dot(A, B)\n",
        "    norma = np.linalg.norm(A)\n",
        "    normb = np.linalg.norm(B)\n",
        "    cos = dot / (norma * normb)\n",
        "\n",
        "    return cos"
      ],
      "metadata": {
        "id": "DAADPk3ih9dD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# this gives you a similar tweet as your input.\n",
        "# this implementation is vectorized...\n",
        "idx = np.argmax(cosine_similarity(document_vecs, tweet_embedding))\n",
        "print(idx)\n",
        "print(all_tweets[idx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3XIzhJehqh2",
        "outputId": "1521bace-a331-4000-96a3-8e37cc41eb09"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4798\n",
            "Save your money \n",
            "Or use them in a good way\n",
            "Money is money \n",
            "It's for better life\n",
            "And fun :)\n",
            "Have a nice day\n",
            "\n",
            "11:11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the most similar tweets with LSH\n",
        "# I will now implement locality sensitive hashing (LSH) to identify the most similar tweet.\n",
        "\n",
        "# Instead of looking at all 10,000 vectors, we can just search a subset to find its nearest neighbors.\n",
        "# we can divide the vector space into regions and search within one region for nearest neighbors of a given vector.# "
      ],
      "metadata": {
        "id": "PXnubza5c8oZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_VECS = len(all_tweets)       # This many vectors.\n",
        "N_DIMS = len(ind2Tweet[1])     # Vector dimensionality.\n",
        "print(f\"Number of vectors is {N_VECS} and each has {N_DIMS} dimensions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lzt8bLCeeH8T",
        "outputId": "d4231628-2af7-4e10-bc84-81daa3a05d8b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of vectors is 10000 and each has 300 dimensions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Choosing the number of planes\n",
        "\n",
        "# * Each plane divides the space to 2 parts.\n",
        "# * So n planes divide the space into 2^{n} hash buckets.\n",
        "# * We want to organize 10,000 document vectors into buckets so that every bucket has about ~16 vectors.\n",
        "# * For that we need {16}/{10000}=625 buckets.\n",
        "# * We're interested in n, number of planes, so that 2^{n}= 625. Now, we can calculate n=log_{2}625 = 9.29 - approx 10"
      ],
      "metadata": {
        "id": "Y7Yn0f5MeND-"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The number of planes. We use log2(256) to have ~16 vectors/bucket.\n",
        "N_PLANES = 10\n",
        "# Number of times to repeat the hashing to improve the search.\n",
        "N_UNIVERSES = 25"
      ],
      "metadata": {
        "id": "LcgvMDnKfDSv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the hash number for a vector\n",
        "# For each vector, we need to get a unique number associated to that vector in order to assign it to a \"hash bucket\".\n",
        "\n",
        "# Hyperlanes in vector spaces\n",
        "# In  3-dimensional vector space, the hyperplane is a regular plane. In  2  dimensional vector space, the hyperplane is a line.\n",
        "# Generally, the hyperplane is subspace which has dimension  1  lower than the original vector space has.\n",
        "# A hyperplane is uniquely defined by its normal vector.\n",
        "# Normal vector  𝑛  of the plane  𝜋  is the vector to which all vectors in the plane  𝜋  are orthogonal (perpendicular in  3  dimensional case).\n",
        "# Using Hyperplanes to split the vector space\n",
        "# We can use a hyperplane to split the vector space into  2  parts.\n",
        "\n",
        "# All vectors whose dot product with a plane's normal vector is positive are on one side of the plane.\n",
        "# All vectors whose dot product with the plane's normal vector is negative are on the other side of the plane.\n",
        "\n",
        "# Encoding hash buckets\n",
        "# For a vector, we can take its dot product with all the planes, then encode this information to assign the vector to a single hash bucket.\n",
        "# When the vector is pointing to the opposite side of the hyperplane than normal, encode it by 0.\n",
        "# Otherwise, if the vector is on the same side as the normal vector, encode it by 1.\n",
        "# If you calculate the dot product with each plane in the same order for every vector, \n",
        "# you've encoded each vector's unique hash ID as a binary number, like [0, 1, 1, ... 0].# \n",
        "\n",
        "\n",
        "# We've initialized hash table hashes for you. It is list of N_UNIVERSES matrices, each describes its own hash table. \n",
        "#  matrix has N_DIMS rows and N_PLANES columns. Every column of that matrix is a N_DIMS-dimensional normal vector for \n",
        "# each of N_PLANES hyperplanes which are used for creating buckets of the particular hash table.\n",
        "\n",
        "# Exercise: Your task is to complete the function hash_value_of_vector which places vector v in the correct hash bucket.\n",
        "\n",
        "# First multiply your vector v, with a corresponding plane. This will give you a vector of dimension  (1,N_planes) .\n",
        "# You will then convert every element in that vector to 0 or 1.\n",
        "# You create a hash vector by doing the following: if the element is negative, it becomes a 0, otherwise you change it to a 1.\n",
        "# You then compute the unique number for the vector by iterating over N_PLANES\n",
        "# Then you multiply  2𝑖  times the corresponding bit (0 or 1).\n",
        "# You will then store that sum in the variable hash_value."
      ],
      "metadata": {
        "id": "UDupVPczfHm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "planes_l = [np.random.normal(size=(N_DIMS, N_PLANES))\n",
        "            for _ in range(N_UNIVERSES)]"
      ],
      "metadata": {
        "id": "xx8d4jjVf_1I"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hash_value_of_vector(v, planes):\n",
        "    \"\"\"Create a hash for a vector; hash_id says which random hash to use.\n",
        "    Input:\n",
        "        - v:  vector of tweet. It's dimension is (1, N_DIMS)\n",
        "        - planes: matrix of dimension (N_DIMS, N_PLANES) - the set of planes that divide up the region\n",
        "    Output:\n",
        "        - res: a number which is used as a hash for your vector\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # for the set of planes,\n",
        "    # calculate the dot product between the vector and the matrix containing the planes\n",
        "    # remember that planes has shape (300, 10)\n",
        "    # The dot product will have the shape (1,10)    \n",
        "    dot_product = np.dot(v, planes)\n",
        "        \n",
        "    # get the sign of the dot product (1,10) shaped vector\n",
        "    sign_of_dot_product = np.sign(dot_product)\n",
        "\n",
        "    # set h to be false (eqivalent to 0 when used in operations) if the sign is negative,\n",
        "    # and true (equivalent to 1) if the sign is positive (1,10) shaped vector\n",
        "    # if the sign is 0, i.e. the vector is in the plane, consider the sign to be positive\n",
        "    h = sign_of_dot_product >= 0\n",
        "\n",
        "    # remove extra un-used dimensions (convert this from a 2D to a 1D array)\n",
        "    h = np.squeeze(h)\n",
        "\n",
        "    # initialize the hash value to 0\n",
        "    hash_value = 0\n",
        "\n",
        "    n_planes = planes.shape[1]\n",
        "    for i in range(n_planes):\n",
        "        # increment the hash value by 2^i * h_i        \n",
        "        hash_value += (2**i) * h[i]\n",
        "        \n",
        "\n",
        "\n",
        "    # cast hash_value as an integer\n",
        "    hash_value = int(hash_value)\n",
        "\n",
        "    return hash_value"
      ],
      "metadata": {
        "id": "XAAqK1nifozW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "idx = 0\n",
        "planes = planes_l[idx]  # get one 'universe' of planes to test the function\n",
        "vec = np.random.rand(1, 300)\n",
        "print(f\" The hash value for this vector,\",\n",
        "      f\"and the set of planes at index {idx},\",\n",
        "      f\"is {hash_value_of_vector(vec, planes)}\")\n",
        "\n",
        "\n",
        "# Expected Output\n",
        "# The hash value for this vector, and the set of planes at index 0, is 768"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t4V20SogNdG",
        "outputId": "2ec16f75-5c01-4f32-af00-ebf37d2f1d81"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The hash value for this vector, and the set of planes at index 0, is 768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a hash table\n",
        "\n",
        "# Given that you have a unique number for each vector (or tweet), You now want to create a hash table. \n",
        "# You need a hash table, so that given a hash_id, you can quickly look up the corresponding vectors. \n",
        "# This allows you to reduce your search by a significant amount of time.\n",
        "\n",
        "# We have given you the make_hash_table function, which maps the tweet vectors to a bucket and stores the vector there. \n",
        "# It returns the hash_table and the id_table. The id_table allows you know which vector in a certain bucket corresponds to what tweet.\n"
      ],
      "metadata": {
        "id": "Rbfyd4-Vgdkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_hash_table(vecs, planes, hash_value_of_vector=hash_value_of_vector):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        - vecs: list of vectors to be hashed.\n",
        "        - planes: the matrix of planes in a single \"universe\", with shape (embedding dimensions, number of planes).\n",
        "    Output:\n",
        "        - hash_table: dictionary - keys are hashes, values are lists of vectors (hash buckets)\n",
        "        - id_table: dictionary - keys are hashes, values are list of vectors id's\n",
        "                            (it's used to know which tweet corresponds to the hashed vector)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # number of planes is the number of columns in the planes matrix\n",
        "    num_of_planes = len(planes[0])\n",
        "\n",
        "    # number of buckets is 2^(number of planes)    \n",
        "    num_buckets = 2**num_of_planes\n",
        "\n",
        "    # create the hash table as a dictionary.\n",
        "    # Keys are integers (0,1,2.. number of buckets)\n",
        "    # Values are empty lists\n",
        "    hash_table = {i:[] for i in range(num_buckets)}\n",
        "\n",
        "    # create the id table as a dictionary.\n",
        "    # Keys are integers (0,1,2... number of buckets)\n",
        "    # Values are empty lists\n",
        "    id_table = {i:[] for i in range(num_buckets)}\n",
        "\n",
        "    # for each vector in 'vecs'\n",
        "    for i, v in enumerate(vecs):\n",
        "        # calculate the hash value for the vector\n",
        "        h = hash_value_of_vector(v, planes)\n",
        "\n",
        "        # store the vector into hash_table at key h,\n",
        "        # by appending the vector v to the list at key h\n",
        "        hash_table[h].append(v)\n",
        "\n",
        "        # store the vector's index 'i' (each document is given a unique integer 0,1,2...)\n",
        "        # the key is the h, and the 'i' is appended to the list at key h\n",
        "        id_table[h].append(i)\n",
        "\n",
        "\n",
        "\n",
        "    return hash_table, id_table"
      ],
      "metadata": {
        "id": "yvx-uTd6guTG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "planes = planes_l[0]  # get one 'universe' of planes to test the function\n",
        "tmp_hash_table, tmp_id_table = make_hash_table(document_vecs, planes)\n",
        "\n",
        "print(f\"The hash table at key 0 has {len(tmp_hash_table[0])} document vectors\")\n",
        "print(f\"The id table at key 0 has {len(tmp_id_table[0])}\")\n",
        "print(f\"The first 5 document indices stored at key 0 of are {tmp_id_table[0][0:5]}\")\n",
        "\n",
        "\n",
        "# Expected output\n",
        "# The hash table at key 0 has 3 document vectors\n",
        "# The id table at key 0 has 3\n",
        "# The first 5 document indices stored at key 0 of are [3276, 3281, 3282]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pjhg3Aog0Jk",
        "outputId": "a286051e-0f50-4a94-8fba-b91c8c00c4f9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The hash table at key 0 has 3 document vectors\n",
            "The id table at key 0 has 3\n",
            "The first 5 document indices stored at key 0 of are [3276, 3281, 3282]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating all hash tables\n",
        "# You can now hash your vectors and store them in a hash table that would allow you to quickly look up and search for similar vectors. \n",
        "# Run the cell below to create the hashes. By doing so, you end up having several tables which have all the vectors. \n",
        "# Given a vector, you then identify the buckets in all the tables. You can then iterate over the buckets and consider much fewer vectors. \n",
        "# The more buckets you use, the more accurate your lookup will be, but also the longer it will take."
      ],
      "metadata": {
        "id": "TYW3emt3hBGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the hashtables\n",
        "def create_hash_id_tables(n_universes):\n",
        "    hash_tables = []\n",
        "    id_tables = []\n",
        "    for universe_id in range(n_universes):  # there are 25 hashes\n",
        "        print('working on hash universe #:', universe_id)\n",
        "        planes = planes_l[universe_id]\n",
        "        hash_table, id_table = make_hash_table(document_vecs, planes)\n",
        "        hash_tables.append(hash_table)\n",
        "        id_tables.append(id_table)\n",
        "    \n",
        "    return hash_tables, id_tables\n",
        "\n",
        "hash_tables, id_tables = create_hash_id_tables(N_UNIVERSES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pylskXC7hKBW",
        "outputId": "5d54ae5a-bf2f-42be-c280-efcfc26516cb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "working on hash universe #: 0\n",
            "working on hash universe #: 1\n",
            "working on hash universe #: 2\n",
            "working on hash universe #: 3\n",
            "working on hash universe #: 4\n",
            "working on hash universe #: 5\n",
            "working on hash universe #: 6\n",
            "working on hash universe #: 7\n",
            "working on hash universe #: 8\n",
            "working on hash universe #: 9\n",
            "working on hash universe #: 10\n",
            "working on hash universe #: 11\n",
            "working on hash universe #: 12\n",
            "working on hash universe #: 13\n",
            "working on hash universe #: 14\n",
            "working on hash universe #: 15\n",
            "working on hash universe #: 16\n",
            "working on hash universe #: 17\n",
            "working on hash universe #: 18\n",
            "working on hash universe #: 19\n",
            "working on hash universe #: 20\n",
            "working on hash universe #: 21\n",
            "working on hash universe #: 22\n",
            "working on hash universe #: 23\n",
            "working on hash universe #: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# k-Nearest neighbors algorithm\n",
        "# k-Nearest neighbors algorithm\n",
        "\n",
        "# k-NN is a method which takes a vector as input and finds the other vectors in the dataset that are closest to it.\n",
        "# The 'k' is the number of \"nearest neighbors\" to find (e.g. k=2 finds the closest two neighbors).\n",
        "\n",
        "# Inputs:\n",
        "\n",
        "# Vector v,\n",
        "# A set of possible nearest neighbors candidates\n",
        "# k nearest neighbors to find.\n",
        "# The distance metric should be based on cosine similarity.\n",
        "# cosine_similarity function is already implemented and imported for you. \n",
        "# It's arguments are two vectors and it returns the cosine of the angle between them.\n",
        "# Iterate over rows in candidates, and save the result of similarities between current row and vector v in a python list. \n",
        "# Take care that similarities are in the same order as row vectors of candidates.\n",
        "# Now you can use numpy argsort to sort the indices for the rows of candidates."
      ],
      "metadata": {
        "id": "RkvkTTKliZKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nearest_neighbor(v, candidates, k=1, cosine_similarity=cosine_similarity):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      - v, the vector you are going find the nearest neighbor for\n",
        "      - candidates: a set of vectors where we will find the neighbors\n",
        "      - k: top k nearest neighbors to find\n",
        "    Output:\n",
        "      - k_idx: the indices of the top k closest vectors in sorted form\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "    similarity_1 = []\n",
        "\n",
        "    # for each candidate vector...\n",
        "    for row in candidates:\n",
        "        # get the cosine similarity\n",
        "        cos_similarity = cosine_similarity(row, v)\n",
        "\n",
        "        # append the similarity to the list\n",
        "        similarity_1.append(cos_similarity)\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    # sort the similarity list and get the indices of the sorted list    \n",
        "    sorted_ids = np.argsort(similarity_1)\n",
        "    #print(sorted_ids)\n",
        "    # Reverse the order of the sorted_ids array\n",
        "    # sorted_ids = np.flipud(sorted_ids)\n",
        "    #print(sorted_ids)\n",
        "\n",
        "    sorted_ids=sorted_ids[-k:]\n",
        "    sorted_ids=sorted_ids[::-1]\n",
        "    \n",
        "    # get the indices of the k most similar candidate vectors\n",
        "    # k_idx = sorted_ids[:-k+1]\n",
        "    k_idx = sorted_ids[-k:]\n",
        "    #k_idx = sorted_ids[:k+1]\n",
        "    ### END CODE HERE ###\n",
        "    return k_idx"
      ],
      "metadata": {
        "id": "XeHpqIYgipX2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Approximate K-NN\n",
        "\n",
        "# Implement approximate K nearest neighbors using locality sensitive hashing, \n",
        "# to search for documents that are similar to a given document at the index doc_id.\n",
        "\n",
        "# Inputs\n",
        "# doc_id is the index into the document list all_tweets.\n",
        "# v is the document vector for the tweet in all_tweets at index doc_id.\n",
        "# planes_l is the list of planes (the global variable created earlier).\n",
        "# k is the number of nearest neighbors to search for.\n",
        "# num_universes_to_use: to save time, we can use fewer than the total number of available universes. By default, it's set to N_UNIVERSES, which is  25  for this assignment.\n",
        "# hash_tables: list with hash tables for each universe.\n",
        "# id_tables: list with id tables for each universe.\n",
        "# The approximate_knn function finds a subset of candidate vectors that are in the same \"hash bucket\" as the input vector 'v'. Then it performs the usual k-nearest neighbors search on this subset (instead of searching through all 10,000 tweets)."
      ],
      "metadata": {
        "id": "IkCQitH9hRnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def approximate_knn(doc_id, v, planes_l, hash_tables, id_tables, k=1, num_universes_to_use=25, hash_value_of_vector=hash_value_of_vector):\n",
        "    \"\"\"Search for k-NN using hashes.\"\"\"\n",
        "    #assert num_universes_to_use <= N_UNIVERSES\n",
        "\n",
        "    # Vectors that will be checked as possible nearest neighbor\n",
        "    vecs_to_consider_l = list()\n",
        "\n",
        "    # list of document IDs\n",
        "    ids_to_consider_l = list()\n",
        "\n",
        "    # create a set for ids to consider, for faster checking if a document ID already exists in the set\n",
        "    ids_to_consider_set = set()\n",
        "\n",
        "    # loop through the universes of planes\n",
        "    for universe_id in range(num_universes_to_use):\n",
        "\n",
        "        # get the set of planes from the planes_l list, for this particular universe_id\n",
        "        planes = planes_l[universe_id]\n",
        "\n",
        "        # get the hash value of the vector for this set of planes\n",
        "        hash_value = hash_value_of_vector(v, planes)\n",
        "\n",
        "        # get the hash table for this particular universe_id\n",
        "        hash_table = hash_tables[universe_id]\n",
        "\n",
        "        # get the list of document vectors for this hash table, where the key is the hash_value\n",
        "        document_vectors_l = hash_table[hash_value]\n",
        "\n",
        "        # get the id_table for this particular universe_id\n",
        "        id_table = id_tables[universe_id]\n",
        "\n",
        "        # get the subset of documents to consider as nearest neighbors from this id_table dictionary\n",
        "        new_ids_to_consider = id_table[hash_value]\n",
        "\n",
        "\n",
        "\n",
        "        # remove the id of the document that we're searching\n",
        "        if doc_id in new_ids_to_consider:\n",
        "            new_ids_to_consider.remove(doc_id)\n",
        "            print(\n",
        "                f\"removed doc_id {doc_id} of input vector from new_ids_to_search\")\n",
        "\n",
        "        # loop through the subset of document vectors to consider\n",
        "        for i, new_id in enumerate(new_ids_to_consider):\n",
        "\n",
        "            # if the document ID is not yet in the set ids_to_consider...\n",
        "            if new_id not in ids_to_consider_set:\n",
        "                # access document_vectors_l list at index i to get the embedding\n",
        "                document_vector_at_i = document_vectors_l[i]\n",
        "                \n",
        "                # then append it to the list of vectors to consider as possible nearest neighbors\n",
        "                vecs_to_consider_l.append(document_vector_at_i)\n",
        "\n",
        "                # append the new_id (the index for the document) to the list of ids to consider\n",
        "                ids_to_consider_l.append(new_id)\n",
        "\n",
        "                # also add the new_id to the set of ids to consider\n",
        "                # (use this to check if new_id is not already in the IDs to consider)\n",
        "                ids_to_consider_set.add(new_id)\n",
        "\n",
        "\n",
        "\n",
        "    # Now run k-NN on the smaller set of vecs-to-consider.\n",
        "    print(\"Fast considering %d vecs\" % len(vecs_to_consider_l))\n",
        "\n",
        "    # convert the vecs to consider set to a list, then to a numpy array\n",
        "    vecs_to_consider_arr = np.array(vecs_to_consider_l)\n",
        "\n",
        "    # call nearest neighbors on the reduced list of candidate vectors\n",
        "    nearest_neighbor_idx_l = nearest_neighbor(v, vecs_to_consider_arr, k=k)\n",
        "\n",
        "    # Use the nearest neighbor index list as indices into the ids to consider\n",
        "    # create a list of nearest neighbors by the document ids\n",
        "    nearest_neighbor_ids = [ids_to_consider_l[idx]\n",
        "                            for idx in nearest_neighbor_idx_l]\n",
        "\n",
        "    return nearest_neighbor_ids"
      ],
      "metadata": {
        "id": "69-GwmwyhkWM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#document_vecs, ind2Tweet\n",
        "doc_id = 0\n",
        "doc_to_search = all_tweets[doc_id]\n",
        "vec_to_search = document_vecs[doc_id]"
      ],
      "metadata": {
        "id": "QQ9uD4CohvuR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample\n",
        "nearest_neighbor_ids = approximate_knn(doc_id, vec_to_search, planes_l, hash_tables, id_tables, k=3, num_universes_to_use=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M-xO3cRhzZv",
        "outputId": "e5a23bd3-ba61-437e-d4cc-06049b4b0333"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "removed doc_id 0 of input vector from new_ids_to_search\n",
            "removed doc_id 0 of input vector from new_ids_to_search\n",
            "removed doc_id 0 of input vector from new_ids_to_search\n",
            "removed doc_id 0 of input vector from new_ids_to_search\n",
            "removed doc_id 0 of input vector from new_ids_to_search\n",
            "Fast considering 77 vecs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Nearest neighbors for document {doc_id}\")\n",
        "print(f\"Document contents: {doc_to_search}\")\n",
        "print(\"\")\n",
        "\n",
        "for neighbor_id in nearest_neighbor_ids:\n",
        "    print(f\"Nearest neighbor at document id {neighbor_id}\")\n",
        "    print(f\"document contents: {all_tweets[neighbor_id]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENdmuwNmh6Pi",
        "outputId": "df8934f0-6b1e-48a0-9451-63a5a52e5913"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nearest neighbors for document 0\n",
            "Document contents: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "\n",
            "Nearest neighbor at document id 51\n",
            "document contents: #FollowFriday @France_Espana @reglisse_menthe @CCI_inter for being top engaged members in my community this week :)\n",
            "Nearest neighbor at document id 701\n",
            "document contents: With the top cutie of Bohol :) https://t.co/Jh7F6U46UB\n",
            "Nearest neighbor at document id 2140\n",
            "document contents: @PopsRamjet come one, every now and then is not so bad :)\n"
          ]
        }
      ]
    }
  ]
}
